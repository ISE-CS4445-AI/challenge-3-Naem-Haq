{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTUuzCyzRqG"
      },
      "source": [
        "# Challenge 3 - Advanced Classification and Evaluation Metrics\n",
        "\n",
        "Welcome to challenge #3! You know the drill by now i.e., the MCQ section and the code challenge section.\n",
        "\n",
        "Good luck! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROAjx16L5x5Y"
      },
      "source": [
        "## Section 1: Multiple Choice Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRALy2NEz0AS"
      },
      "source": [
        "### Q1. Which statement is **true** regarding **precision** in a multi-class classification context? (1 point)\n",
        "\n",
        "**Options**:  \n",
        "1. Precision is the fraction of actual positives that were predicted positive.  \n",
        "2. Precision is the fraction of predicted positives that are actual positives.  \n",
        "3. Precision is the fraction of negative predictions that are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFtBxSL40CG9",
        "outputId": "49e0c805-cd5f-4d4b-be20-2cf551c8bc86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: Which statement is **true** regarding **precision** in a multi-class classification context? (1 point)\n",
            "Answer: Precision is the fraction of predicted positives that are actual positives\n"
          ]
        }
      ],
      "source": [
        "def answer_q1():\n",
        "    \"\"\"\n",
        "    Q1: Which statement is **true** regarding **precision** in a multi-class classification context? (1 point)\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"Precision is the fraction of actual positives that were predicted positive\",\n",
        "        2: \"Precision is the fraction of predicted positives that are actual positives\",\n",
        "        3: \"Precision is the fraction of negative predictions that are correct\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q1: Which statement is **true** regarding **precision** in a multi-class classification context? (1 point)\\nAnswer: {answer_q1()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9BRj9Qv0v9o"
      },
      "source": [
        "### Q2. Which statement correctly describes the confusion matrix in a multi-class setting? (1 point)\n",
        "**Options:**\n",
        "1. It’s only valid for binary classification, not used for multi-class.\n",
        "2. It’s an NxN array for N classes, comparing predicted vs. actual class labels.\n",
        "3. It only shows the total correct vs. total incorrect predictions, no class breakdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFto7n3ny7UA",
        "outputId": "2025056d-b701-4a72-89b4-7a459c2828b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2: Which statement correctly describes the confusion matrix in a multi-class setting?\n",
            "Answer: It’s an NxN array for N classes, comparing predicted vs. actual class labels\n"
          ]
        }
      ],
      "source": [
        "def answer_q2():\n",
        "    \"\"\"\n",
        "    Q2: Which statement correctly describes the confusion matrix in a multi-class setting? (1 point)\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"It’s only valid for binary classification, not used for multi-class\",\n",
        "        2: \"It’s an NxN array for N classes, comparing predicted vs. actual class labels\",\n",
        "        3: \"It only shows the total correct vs. total incorrect predictions, no class breakdown\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q2: Which statement correctly describes the confusion matrix in a multi-class setting?\\nAnswer: {answer_q2()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYgSO7Z2FTd"
      },
      "source": [
        "### Q3. Which classifier typically uses bagging plus random subsets of features to reduce variance? (1 point)\n",
        "**Options:**\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5SAsK9C2Z8R",
        "outputId": "edd20aeb-bf1e-4fcf-fd84-17c8e39a92a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q3: Which classifier typically uses bagging plus random subsets of features to reduce variance?\n",
            "Answer: Random Forest\n"
          ]
        }
      ],
      "source": [
        "def answer_q3():\n",
        "    \"\"\"\n",
        "    Q3: Which classifier typically uses bagging plus random subsets of features to reduce variance? (1 point)\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"Decision Tree\",\n",
        "        2: \"Random Forest\",\n",
        "        3: \"Naive Bayes\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q3: Which classifier typically uses bagging plus random subsets of features to reduce variance?\\nAnswer: {answer_q3()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. If we see a high recall but low precision for a certain class, what does that usually imply? (1 point)\n",
        "**Options:**\n",
        "1. The model rarely finds actual positives for that class.\n",
        "2. The model finds most actual positives (few false negatives), but also mislabels many negatives as that class (many false positives).\n",
        "3. The model completely fails to detect that class at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q3: If we see a high recall but low precision for a certain class, what does that usually imply?\n",
            "Answer: The model finds most actual positives (few false negatives), but also mislabels many negatives as that class (many false positives)\n"
          ]
        }
      ],
      "source": [
        "def answer_q4():\n",
        "    \"\"\"\n",
        "    Q3: If we see a high recall but low precision for a certain class, what does that usually imply? (1 point)\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"The model rarely finds actual positives for that class\",\n",
        "        2: \"The model finds most actual positives (few false negatives), but also mislabels many negatives as that class (many false positives)\",\n",
        "        3: \"The model completely fails to detect that class at all\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q3: If we see a high recall but low precision for a certain class, what does that usually imply?\\nAnswer: {answer_q4()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tw01le53fJ"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Code challenge (5 points)\n",
        "\n",
        "For this code challenge, you will work with the [UCI Car Evaluation dataset](https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set). A summary of the dataset is as follows.\n",
        "\n",
        "### Features:\n",
        "- **buying**: Buying price of the car (vhigh, high, med, low)\n",
        "- **maint**: Price of the maintenance (vhigh, high, med, low)\n",
        "- **doors**: Number of doors (2, 3, 4, 5more)\n",
        "- **persons**: Capacity in terms of persons to carry (2, 4, more)\n",
        "- **lug_boot**: Size of the luggage boot (small, med, big)\n",
        "- **safety**: Estimated safety of the car (low, med, high)\n",
        "\n",
        "### Class Labels:\n",
        "- **unacc**: Unacceptable\n",
        "- **acc**: Acceptable\n",
        "- **good**: Good\n",
        "- **vgood**: Very Good\n",
        "\n",
        "To pass this section of the weekly challenge, uncomment/fill in code where necessary (marked with a 'TODO:' comment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression  # Example classifier\n",
        "from sklearn.naive_bayes import GaussianNB  # Alternative classifier\n",
        "from sklearn.ensemble import RandomForestClassifier  # Uncomment if using Random Forest\n",
        "\n",
        "# Metrics & Evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Visualization \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Load & Encode Car Dataset (1 point)\n",
        "\n",
        "**Goals** for `load_and_encode_car_data()`\n",
        "1. Reads car.csv.\n",
        "2. Encodes 6 features (buying, maint, doors, persons, lug_boot, safety).\n",
        "    e.g. one-hot or custom mapping.\n",
        "3. Maps class -> integer: unacc=0, acc=1, good=2, vgood=3.\n",
        "4. Returns (X, y) as arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Cm3MG7iySn15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (1729, 27) (1729,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_and_encode_car_data():\n",
        "    \"\"\"\n",
        "    1) Reads 'car.csv'\n",
        "    2) Encodes the 6 categorical features using One-Hot Encoding\n",
        "    3) Maps class labels to {0,1,2,3}\n",
        "    4) Returns X (features) and y (target)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(\"car.csv\", header=None, \n",
        "                     names=[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"])\n",
        "    \n",
        "    # Encode categorical features using OneHotEncoder\n",
        "    feature_cols = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    X = encoder.fit_transform(df[feature_cols])\n",
        "    \n",
        "    # Encode target class labels to {0,1,2,3}\n",
        "    label_mapping = {\"unacc\": 0, \"acc\": 1, \"good\": 2, \"vgood\": 3}\n",
        "    y = df[\"class\"].map(label_mapping).values  # Ensure correct mapping\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Demonstration\n",
        "X, y = load_and_encode_car_data()\n",
        "print(\"Shapes:\", X.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hry_mfD2St36"
      },
      "source": [
        "### 2. Train Baseline Model (1 point)\n",
        "\n",
        "**Goals** for `train_baseline_model(X, y)`:\n",
        "1. Split data (test_size=0.2, random_state=42).\n",
        "2. Use a simple classifier (logistic or naive_bayes, etc.).\n",
        "3. Return (model, X_test, y_test, y_pred)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "t6L9tk63SxuZ"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[116], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline Model Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, X_test, y_test, y_pred\n\u001b[1;32m---> 25\u001b[0m model, X_test, y_test, y_pred \u001b[38;5;241m=\u001b[39m train_classifier(X, y)\n",
            "Cell \u001b[1;32mIn[116], line 15\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)  \u001b[38;5;66;03m# Alternative: model = GaussianNB()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1185\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1184\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1185\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1186\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Naem Haq\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "def train_classifier(X, y):\n",
        "    \"\"\"\n",
        "    1. Split X, y (test_size=0.2, random_state=42)\n",
        "    2. Pick a classifier (Logistic Regression or Naive Bayes)\n",
        "    3. Fit & predict\n",
        "    4. Return (model, X_test, y_test, y_pred)\n",
        "    \"\"\"\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Choose a classifier (Logistic Regression or Naive Bayes)\n",
        "    model = LogisticRegression(max_iter=200)  # Alternative: model = GaussianNB()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Print accuracy\n",
        "    print(\"Baseline Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    return model, X_test, y_test, y_pred\n",
        "\n",
        "model, X_test, y_test, y_pred = train_classifier(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Generate Confusion Matrix (1 point)\n",
        "\n",
        "**Goals**:\n",
        "1.  `generate_confmatrix(y_true, y_pred)` → returns an NxN confusion matrix array from scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 66   4   8   0]\n",
            " [  7   4   0   0]\n",
            " [  6   0 236   0]\n",
            " [  3   1   0  11]]\n"
          ]
        }
      ],
      "source": [
        "def generate_confmatrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Return confusion_matrix array (sklearn).\n",
        "    \"\"\"\n",
        "    return confusion_matrix(y_true, y_pred)\n",
        "\n",
        "conf_matrix = generate_confmatrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww-oRhCNS0_6"
      },
      "source": [
        "### 4. Compute F1 score (1 point)\n",
        "\n",
        "**Goals** for `manual_f1_for_class(y_true, y_pred, class_idx)`:\n",
        "1. Use scikit-learn to get precision, recall for that single class, e.g. by calling `precision_score(..., labels=[class_idx], average='macro')` or `average='binary'` approach.\n",
        "2. Then manually compute F1 using the formula (no direct scikit calls for F1).\n",
        "3. Return the F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score for class 2: 0.39999999999999997\n"
          ]
        }
      ],
      "source": [
        "def manual_f1_for_class(y_true, y_pred, class_idx):\n",
        "    \"\"\"\n",
        "    1) Use scikit-learn to get precision, recall for 'class_idx' only\n",
        "    2) Compute F1 score manually\n",
        "    3) Return that float\n",
        "    \"\"\"\n",
        "    # Calculate precision and recall for the specific class\n",
        "    precision = precision_score(y_true, y_pred, labels=[class_idx], average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, labels=[class_idx], average='macro', zero_division=0)\n",
        "\n",
        "    # Compute F1 score manually\n",
        "    if precision + recall == 0:\n",
        "        f1_score = 0.0  # Avoid division by zero\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "f1 = manual_f1_for_class(y_test, y_pred, class_idx=2)\n",
        "print(\"F1 Score for class 2:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaVV4aDxYEUH"
      },
      "source": [
        "### 5. Generate Classification Report (1 point)\n",
        "\n",
        "**Goals**\n",
        "\n",
        "1. Generate a classification report for your trained model and return the report (string value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model Accuracy: 0.9161849710982659\n",
            "Generate confmatrix:\n",
            "[[ 66   4   8   0]\n",
            " [  7   4   0   0]\n",
            " [  6   0 236   0]\n",
            " [  3   1   0  11]]\n",
            "F1 for class 0 manually: 0.8250000000000001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.85      0.83        78\n",
            "           2       0.44      0.36      0.40        11\n",
            "           3       0.97      0.98      0.97       242\n",
            "           4       1.00      0.73      0.85        15\n",
            "\n",
            "    accuracy                           0.92       346\n",
            "   macro avg       0.80      0.73      0.76       346\n",
            "weighted avg       0.92      0.92      0.91       346\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calls sklearn's classification_report\n",
        "    Returns it as a string\n",
        "    \"\"\"\n",
        "    return classification_report(y_true, y_pred)\n",
        "\n",
        "# Optional demonstration\n",
        "X, y = load_and_encode_car_data()\n",
        "model, X_te, y_te, y_pr = train_classifier(X, y)\n",
        "print(\"Generate confmatrix:\")\n",
        "cm = generate_confmatrix(y_te, y_pr)\n",
        "print(cm)\n",
        "f1class0 = manual_f1_for_class(y_te, y_pr, class_idx=0)\n",
        "print(\"F1 for class 0 manually:\", f1class0)\n",
        "rep = generate_classification_report(y_te, y_pr)\n",
        "print(rep)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
